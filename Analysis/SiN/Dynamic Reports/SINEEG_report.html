<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Gorka Fraga-Gonzalez">
<meta name="author" content="Samuel Mueller">
<meta name="author" content="Alexis Hervais-Adelman">
<meta name="dcterms.date" content="2024-01-27">

<title>Data Report: An EEG experiment on Speech-in-Noise Comprehension</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="SINEEG_report_files/libs/clipboard/clipboard.min.js"></script>
<script src="SINEEG_report_files/libs/quarto-html/quarto.js"></script>
<script src="SINEEG_report_files/libs/quarto-html/popper.min.js"></script>
<script src="SINEEG_report_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="SINEEG_report_files/libs/quarto-html/anchor.min.js"></script>
<link href="SINEEG_report_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="SINEEG_report_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="SINEEG_report_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="SINEEG_report_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="SINEEG_report_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#abstract" id="toc-abstract" class="nav-link active" data-scroll-target="#abstract">Abstract</a></li>
  <li><a href="#methods" id="toc-methods" class="nav-link" data-scroll-target="#methods">Methods</a>
  <ul class="collapse">
  <li><a href="#participants-and-procedures" id="toc-participants-and-procedures" class="nav-link" data-scroll-target="#participants-and-procedures">Participants and procedures</a></li>
  <li><a href="#experimental-task" id="toc-experimental-task" class="nav-link" data-scroll-target="#experimental-task">Experimental Task</a>
  <ul class="collapse">
  <li><a href="#trial-design" id="toc-trial-design" class="nav-link" data-scroll-target="#trial-design">Trial design</a></li>
  <li><a href="#blocks-design" id="toc-blocks-design" class="nav-link" data-scroll-target="#blocks-design">Blocks design</a></li>
  <li><a href="#stimuli" id="toc-stimuli" class="nav-link" data-scroll-target="#stimuli">Stimuli</a></li>
  <li><a href="#speech-generation" id="toc-speech-generation" class="nav-link" data-scroll-target="#speech-generation">Speech generation</a></li>
  <li><a href="#trimming-silences" id="toc-trimming-silences" class="nav-link" data-scroll-target="#trimming-silences">Trimming silences</a></li>
  <li><a href="#reading-the-target-word-onsets-and-offsets" id="toc-reading-the-target-word-onsets-and-offsets" class="nav-link" data-scroll-target="#reading-the-target-word-onsets-and-offsets">Reading the target word onsets and offsets</a></li>
  </ul></li>
  <li><a href="#resting-state" id="toc-resting-state" class="nav-link" data-scroll-target="#resting-state">Resting-state</a></li>
  <li><a href="#eeg" id="toc-eeg" class="nav-link" data-scroll-target="#eeg">EEG</a>
  <ul class="collapse">
  <li><a href="#signal-acquisition" id="toc-signal-acquisition" class="nav-link" data-scroll-target="#signal-acquisition">Signal acquisition</a></li>
  <li><a href="#minimal-preprocessing-of-source-data-and-event-trigger-correction" id="toc-minimal-preprocessing-of-source-data-and-event-trigger-correction" class="nav-link" data-scroll-target="#minimal-preprocessing-of-source-data-and-event-trigger-correction">Minimal preprocessing of source data and event trigger correction</a></li>
  <li><a href="#preprocessing" id="toc-preprocessing" class="nav-link" data-scroll-target="#preprocessing">Preprocessing</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#potential-secondary-analyses" id="toc-potential-secondary-analyses" class="nav-link" data-scroll-target="#potential-secondary-analyses">Potential secondary analyses</a>
  <ul class="collapse">
  <li><a href="#exploring-data-qualities-and-minimal-preprocessing" id="toc-exploring-data-qualities-and-minimal-preprocessing" class="nav-link" data-scroll-target="#exploring-data-qualities-and-minimal-preprocessing">Exploring data qualities and minimal preprocessing</a></li>
  <li><a href="#utilizing-the-resting-state-recordings" id="toc-utilizing-the-resting-state-recordings" class="nav-link" data-scroll-target="#utilizing-the-resting-state-recordings">Utilizing the resting-state recordings</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Data Report: An EEG experiment on Speech-in-Noise Comprehension</h1>
<p class="subtitle lead">Pilot EEG study on using a sentence matrix comprehension task with noise-vocoded speech and speech in speech-shaped noise <a href="https://github.com/Neuroling/SPINCO_SINEEG">https://github.com/Neuroling/SPINCO_SINEEG</a></p>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author">Gorka Fraga-Gonzalez <a href="https://orcid.org/0000-0002-1857-8607" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Neurolinguistics group, University of Zurich
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">Samuel Mueller </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Neurolinguistics group, University of Zurich
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    <p class="author">Alexis Hervais-Adelman <a href="https://orcid.org/0000-0002-5232-626X" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Neurolinguistics group, University of Zurich
          </p>
        <p class="affiliation">
            Department of fundamental neurosciences, University of Geneva
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 27, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="abstract" class="level1">
<h1>Abstract</h1>
Hearing loss is becoming an increasingly common complaint, not solely due to aging populations, but also as a result of increasing incidence of noise-induced hearing loss in younger individuals <span class="citation" data-cites="niskar2001estimated">Niskar et al. (<a href="#ref-niskar2001estimated" role="doc-biblioref">2001</a>)</span> and <span class="citation" data-cites="shargorodsky2010change">Shargorodsky et al. (<a href="#ref-shargorodsky2010change" role="doc-biblioref">2010</a>)</span>. One of the major consequences is reduced speech-comprehension ability, especially in acoustically challenging conditions such as in reverberant or noisy environments. Individual differences in (dis)satisfaction with both hearing aids and cochlear implants calls for more investigation on interventions that could enhance speech comprehension abilities by exploiting existing cognitive mechanisms. To this end, this project aims at gaining fundamental knowledge on the neurocognitive basis of speech comprehension in acoustically challenging situations.
<p>
</p><p>Our experiment uses electroencephalography (EEG) to investigate patterns of electrophysiological activity associated with better speech in noise comprehension. This pilot study has data from 15 healthy adult participants performing a sentence comprehension task with noise-vocoded speech and speech in speech-shaped noise stimuili. We use a multivariate pattern analysis approach focused on using EEG information preceding stimuli to decode accuracy of performance.</p>
<p>This pilot study is aimed at specifying targets and experimental design for subsequent studies focused on potential neurofeedback interventions</p>
<blockquote class="blockquote">
<p>NOTE: some of the links in this report use the permalink to scripts in the github repository. If any of these links is found to be broken, please contact the authors.</p>
</blockquote>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<section id="participants-and-procedures" class="level2">
<h2 class="anchored" data-anchor-id="participants-and-procedures">Participants and procedures</h2>
<p>We recruited 15 adult participants using a mailing list of the Faculty of Psychology (University of Zurich) and flyers. Inclusion criteria were: age 18-35 years, normmal hearing without hearing aids, normal vision (corrections allowed), German as first language, right-handed. Exclusion criteria were being fuent in more than two languages, having hearing aids or history of hearing impairments or having any psychological or psychiatric condition that may impede adequate task performance (e.g., attentional problems, speech comprehension issues). All participants signed an inform consent form before participation with included a 60 CHF reward for participation. The study was approved by the ethics commision of the canton of Zurich.</p>
<p>XX were excluded due to…Data from x participant was discarded…</p>
<p>Before selection for the experiment participants were screened via phone call or email on the main inclusion criteria for the study. Upon arrival to the lab participants first filled in a short questionnaire with a additional questions on language hearing and an relevant psychological or psychiatric condition that may impact task performance. The questionnaire was presented using the web application <a href="https://www.project-redcap.org/">Redcap</a> to ensure that any personal data was saved in a secure environment. [[[<strong><em>Enter what variables were masked and list of variables in the appendix </em></strong>]]]. After completion of the pilot the survey data was downloaded as a table with identifying variables like D.O.B masked.</p>
<p>After completing the questionnaires participants were informed about the specific content of the session. Electrode placement and signal check took approximately between 20 and 40 minutes. Then a few example trials were presented before starting the session consisting of: a 4 minutes resting state recording with eyes closed, the main task (with 4 blocks of approximately 15 minuntes duration each) and finally, another 4 minutes of eyes closed resting state.</p>
</section>
<section id="experimental-task" class="level2">
<h2 class="anchored" data-anchor-id="experimental-task">Experimental Task</h2>
<section id="trial-design" class="level3">
<h3 class="anchored" data-anchor-id="trial-design">Trial design</h3>
<p>We used a version of the Coordinate Response Measure (CRM) task used in brungart2001evaluation. In the task, participants hear different versions of a setence with a fixed structure and three words that change in every trial. There are 4 possible alternatives for each of the targets, thus there are 64 possible variations of the sentences. The task was originally designed to investigate intelligibility of speech commands (in a military context) in audio transmissions with various types of background competition. The German sentence used in this study was:</p>
<blockquote class="blockquote">
<p>“Vorsicht [call sign], geh sofort zum [color] Feld von der Spalte [number]”</p>
</blockquote>
<p>(Translated with a small variation from the original article in English: “<em>Ready [call sign] go to [color][number] now</em>”). The 4 possible call sign items were: Adler, Drossel, Tiger or Kröte (eagle, thrush, tiger or toad). The possible colors were gelben, gruenen, roten, weissen (yellow, green, red or white). The numbers were Ein, Zwei, Drei or Vier (one to four).</p>
<pre><code>    [[[ insert screenshot about here ]]] </code></pre>
<p>In each trial the sentences were aurally presented and a fixation cross was shown in the center of the screen. After playing the sentence, an array of 4 x 3 was shown with drawings representing the target items, the array had always in the same order and columns 1-3 represented targets 1-3 in the same order as they appeared in the sentence, that is, first the column with call signs, then colors and then numbers. Participants had to click with the mouse on the picture representing the item they heard in each of the columns before moving on to the next trial. The trials were response-terminated, but there was a response time limit of 10 seconds before moving on to the next trial. In the example trials, presented with 3 different levels of auditory difficulty the researchers conducting the experiment ensured that participants understood that they should click on each of the three columns in all trials, even if they were not sure what item they heard.</p>
</section>
<section id="blocks-design" class="level3">
<h3 class="anchored" data-anchor-id="blocks-design">Blocks design</h3>
<p>The sentences were presented in blocks of two audio manipulation conditions: noise vocoded (NV) or with a background speech-shaped noise (SSN) and there were three possible levels of difficulty. Half of the sentences were synthesized with a female voice and half with a male voice. The <em>Stimuli</em> section provides more details on these manipulations.</p>
<p>The task consisted of 4 blocks of trials: two blocks of NV and two blocks of SSN stimuli presented in alternating order. There were four different block orders (all possible block sequences without consecutive blocks of the same condition, e.g., NV1-SSN1-NV2-SSN2). Participants were randomly assigned to these sequences [[[[[[CHECK - ref to code]]]]]. Within in each block the distribution of voices and levels of difficulty [[[[…]]]</p>
</section>
<section id="stimuli" class="level3">
<h3 class="anchored" data-anchor-id="stimuli">Stimuli</h3>
</section>
<section id="speech-generation" class="level3">
<h3 class="anchored" data-anchor-id="speech-generation">Speech generation</h3>
<p>We used the <a href="https://cloud.google.com/text-to-speech/">Google Cloud Text-to-speech API</a> to generate the 64 possible variations of our sentence in a female and male speaker voice. Each sentence was output in a separate .wav file with 48KHz sampling rate. After trying several of the freely available voices we chose the <a href="https://cloud.google.com/text-to-speech/docs/voice-types#neural2_voices">neural2 voices</a> ‘de-DE-Neural2-D’ and ‘de-DE-Neural2-F’ for male and female speech, respective, as they sounded more natural (this was verified by native speakers). We used a Python script to communicate with the text-to-speech API (see<br>
<a href="https://github.com/Neuroling/SPINCO_SINEEG/blob/65c604a2246d26b94cdd150298f1dd7150bd728f/Gen_stimuli/Gen_speech/TTS_sentenceGenerator.py">Gen_stimuli script</a> in our project repository)</p>
</section>
<section id="trimming-silences" class="level3">
<h3 class="anchored" data-anchor-id="trimming-silences">Trimming silences</h3>
<pre><code>...</code></pre>
</section>
<section id="reading-the-target-word-onsets-and-offsets" class="level3">
<h3 class="anchored" data-anchor-id="reading-the-target-word-onsets-and-offsets">Reading the target word onsets and offsets</h3>
<p>The onsets and offsets times of the target items for each of the audio files with the sentences was dectected following a semiautomatic approach. We used the freely available phonetics tool <a href="https://www.fon.hum.uva.nl/praat/">Praat</a></p>
<section id="noise-vocoding" class="level4">
<h4 class="anchored" data-anchor-id="noise-vocoding">Noise vocoding</h4>
<p>We used the approach described in <span class="citation" data-cites="aller2022differential">Aller et al. (<a href="#ref-aller2022differential" role="doc-biblioref">2022</a>)</span>, and modified from <span class="citation" data-cites="zoefel2023intelligibility">Zoefel, Gilbert, and Davis (<a href="#ref-zoefel2023intelligibility" role="doc-biblioref">2023</a>)</span>.</p>
</section>
<section id="speech-in-speech-shaped-noise" class="level4">
<h4 class="anchored" data-anchor-id="speech-in-speech-shaped-noise">Speech in speech-shaped noise</h4>
</section>
</section>
</section>
<section id="resting-state" class="level2">
<h2 class="anchored" data-anchor-id="resting-state">Resting-state</h2>
<p>The resting-state recordings before and after the task were identical. Subjects were instructed to close their eyes and remain still for 4 minutes. The start and end of the 4 minutes block was indicated by a beep sound (1-second long).</p>
</section>
<section id="eeg" class="level2">
<h2 class="anchored" data-anchor-id="eeg">EEG</h2>
<section id="signal-acquisition" class="level3">
<h3 class="anchored" data-anchor-id="signal-acquisition">Signal acquisition</h3>
<p>The EEG recordings were conducted using the Biosemi Active Two MK2hS-System with a sampling rate of 2048 Hz. We used 64 scalp electrodes positioned according to the 10-20 system. In addition, six external Flat-Type Active electrodes were used: four electrodes for recording the vertical and horizontal electro-oculogram (EOG) and two electrodes were placed at mastoids for off-line reference. The Biosemi system uses two additional electrodes (Common Mode Sense [CMS] and Driven Right Leg [DRL]) creating a feedback loop to replace the conventional ground electrode (see www.biosemi.com/faq/cms&amp;drl.htm for details). The CMS electrode served as online reference. The offset of the electrodes was kept between +20 and -20 µV.</p>
</section>
<section id="minimal-preprocessing-of-source-data-and-event-trigger-correction" class="level3">
<h3 class="anchored" data-anchor-id="minimal-preprocessing-of-source-data-and-event-trigger-correction">Minimal preprocessing of source data and event trigger correction</h3>
<p>The source data directly obtained from the recordings underwent minimal preprocessing and correction of event triggers in order to generate the raw datasets used for further preprocessing and analysis.</p>
<p>During the experimental session a single file was generated containing the resting-state recordings as well as the main task. The file was in Biosemi’s <a href="https://www.biosemi.com/faq/file_format.htm">bdf</a> format which is a 24 bits version of the popular [EDF](https://www.edfplus.info/#:~:text=European%20Data%20Format%20(EDF)&amp;text=The%20European%20Data%20Format%20(EDF,international%20Sleep%20Congress%20in%20Copenhagen.) format. Using EEGlab we</p>
</section>
<section id="preprocessing" class="level3">
<h3 class="anchored" data-anchor-id="preprocessing">Preprocessing</h3>
<p>We opted for a fully automated pipeline implemented with the open-source Matlab toolbox <a href="https://github.com/methlabUZH/automagic">automagic</a>, see (<span class="citation" data-cites="pedroni2019automagic">Pedroni, Bahreini, and Langer (<a href="#ref-pedroni2019automagic" role="doc-biblioref">2019</a>)</span>). The toolbox is actually a wrapper that runs different functions from EEGlab (<span class="citation" data-cites="delorme2004eeglab">Delorme and Makeig (<a href="#ref-delorme2004eeglab" role="doc-biblioref">2004</a>)</span>), probably the most widely used open-source software for EEG analysis (also Matlab-based). Automagic facilitates selection of multiple options for data cleaning and artifact rejection available in different plugins for EEGlab. In addition, it provides its own data quality summary measures. We chose Automagic as our main preprocessing approach first to facilitate automation, potentially iterating through different pipeline variations and second, to obtain more <em>objective</em> descriptors of data quality, constraining the researchers’ degrees of freedom and facilitating transparency on subsequent decisions and explorations on data quality levels vis-a-vis main analyeses results.</p>
</section>
</section>
</section>
<section id="potential-secondary-analyses" class="level1">
<h1>Potential secondary analyses</h1>
<section id="exploring-data-qualities-and-minimal-preprocessing" class="level2">
<h2 class="anchored" data-anchor-id="exploring-data-qualities-and-minimal-preprocessing">Exploring data qualities and minimal preprocessing</h2>
<ul>
<li><p>Neurofeedback studies deal with very noisy data in real-time and this requires some basic preprocessing to ensure we are targetting the intenteded neural processes, or at least approximating them as much as possible. Thus a potential question is how different preprocessing pipelines may affect our main analysis results (e.g., multivariate pattern analysis and classification)</p></li>
<li><p>A related question is how individual differences in data quality before/after preprocessing may affect performance of classifiers</p></li>
</ul>
</section>
<section id="utilizing-the-resting-state-recordings" class="level2">
<h2 class="anchored" data-anchor-id="utilizing-the-resting-state-recordings">Utilizing the resting-state recordings</h2>
<ul>
<li><p>Explore individual alpha peaks (IAF) in resting state recordings. Use IAF-based band as target for decoding. If neurofeedback settings are to work out, these individual differences would be important. For example, we may not be able to adequately capture alpha activity in all subjects, due to SNR or to actual differences in oscillatory activity. Here the emphasis is on alpha as an example of typically clear feature, but this may apply to other features or frequency ranges.</p></li>
<li><p>Explore consistency of IAF (before and after task). This can give an impression of robustness of features like individual alpha peak and by extension of using power averaged by the classical frequency bands.</p></li>
<li><p>Explore correlations between resting state power and task performance</p></li>
</ul>

</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-aller2022differential" class="csl-entry" role="listitem">
Aller, Máté, Heidi Solberg Økland, Lucy J MacGregor, Helen Blank, and Matthew H Davis. 2022. <span>“Differential Auditory and Visual Phase-Locking Are Observed During Audio-Visual Benefit and Silent Lip-Reading for Speech Perception.”</span> <em>Journal of Neuroscience</em> 42 (31): 6108–20.
</div>
<div id="ref-delorme2004eeglab" class="csl-entry" role="listitem">
Delorme, Arnaud, and Scott Makeig. 2004. <span>“EEGLAB: An Open Source Toolbox for Analysis of Single-Trial EEG Dynamics Including Independent Component Analysis.”</span> <em>Journal of Neuroscience Methods</em> 134 (1): 9–21.
</div>
<div id="ref-niskar2001estimated" class="csl-entry" role="listitem">
Niskar, Amanda Sue, Stephanie M Kieszak, Alice E Holmes, Emilio Esteban, Carol Rubin, and Debra J Brody. 2001. <span>“Estimated Prevalence of Noise-Induced Hearing Threshold Shifts Among Children 6 to 19 Years of Age: The Third National Health and Nutrition Examination Survey, 1988–1994, United States.”</span> <em>Pediatrics</em> 108 (1): 40–43.
</div>
<div id="ref-pedroni2019automagic" class="csl-entry" role="listitem">
Pedroni, Andreas, Amirreza Bahreini, and Nicolas Langer. 2019. <span>“Automagic: Standardized Preprocessing of Big EEG Data.”</span> <em>NeuroImage</em> 200: 460–73.
</div>
<div id="ref-shargorodsky2010change" class="csl-entry" role="listitem">
Shargorodsky, Josef, Sharon G Curhan, Gary C Curhan, and Roland Eavey. 2010. <span>“Change in Prevalence of Hearing Loss in US Adolescents.”</span> <em>Jama</em> 304 (7): 772–78.
</div>
<div id="ref-zoefel2023intelligibility" class="csl-entry" role="listitem">
Zoefel, Benedikt, Rebecca A Gilbert, and Matthew H Davis. 2023. <span>“Intelligibility Improves Perception of Timing Changes in Speech.”</span> <em>Plos One</em> 18 (1): e0279024.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>