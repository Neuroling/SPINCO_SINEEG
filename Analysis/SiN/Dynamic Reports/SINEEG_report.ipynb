{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: |-\n",
    "  Data Report: An EEG experiment on Speech-in-Noise Comprehension\n",
    "subtitle: Pilot EEG study on using a sentence matrix comprehension task with noise-vocoded speech and speech in speech-shaped noise [https://github.com/Neuroling/SPINCO_SINEEG](https://github.com/Neuroling/SPINCO_SINEEG) \n",
    "date: last-modified\n",
    "author:\n",
    "  - name: Gorka Fraga-Gonzalez\n",
    "    orcid: 0000-0002-1857-8607\n",
    "    email: gorka.fragagonzalez@uzh.ch\n",
    "    affiliation:\n",
    "      - name: 'Neurolinguistics group, University of Zurich'      \n",
    "  - name: Samuel Mueller \n",
    "    email: samuel.mueller@psychologie.uzh.ch\n",
    "    affiliation:\n",
    "      - name: 'Neurolinguistics group, University of Zurich' \n",
    "  - name: Alexis Hervais-Adelman  \n",
    "    orcid: 0000-0002-5232-626X\n",
    "    email: alexis.hervais-adelman@psychologie.uzh.ch\n",
    "    affiliation:\n",
    "      - name: 'Neurolinguistics group, University of Zurich'\n",
    "      - name: 'Department of fundamental neurosciences, University of Geneva'\n",
    "\n",
    "bibliography: references.bib\n",
    "format:\n",
    "  html:  \n",
    "    toc: true\n",
    "    code-fold: true\n",
    "    code-link: true    \n",
    "    code-copy: true\n",
    "    code-tools: true\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "Hearing loss is becoming an increasingly common complaint, not solely due to aging populations, but also as a result of increasing incidence of noise-induced hearing loss in younger individuals @niskar2001estimated and @shargorodsky2010change. One of the major consequences is reduced speech-comprehension ability, especially in acoustically challenging conditions such as in reverberant or noisy environments. Individual differences in (dis)satisfaction with both hearing aids and cochlear implants calls for more investigation on interventions that could enhance speech comprehension abilities by exploiting existing cognitive mechanisms. To this end, this project aims at gaining fundamental knowledge on the neurocognitive basis of speech comprehension in acoustically challenging situations.\n",
    "<p>Our experiment uses electroencephalography (EEG) to investigate patterns of electrophysiological activity associated with better speech in noise comprehension. This pilot study has data from 15 healthy adult participants performing a sentence comprehension task with noise-vocoded speech and speech in speech-shaped noise stimuili. We use a multivariate pattern analysis approach focused on using EEG information preceding stimuli to decode accuracy of performance.\n",
    "    \n",
    "This pilot study is aimed at specifying targets and experimental design for subsequent studies focused on potential neurofeedback interventions\n",
    "    \n",
    "> NOTE: some of the links in this report use the permalink to scripts in the github repository. If any of these links is found to be broken, please contact the authors.   \n",
    "    \n",
    "# Methods \n",
    "    \n",
    "## Participants and procedures\n",
    "\n",
    "We recruited 15 adult participants using a mailing list of the Faculty of Psychology (University of Zurich) and flyers. Inclusion criteria were: age 18-35 years, normmal hearing without hearing aids, normal vision (corrections allowed), German as first language, right-handed. Exclusion criteria were being fuent in more than two languages, having hearing aids or history of hearing impairments or having any psychological or psychiatric condition that may impede adequate task performance (e.g., attentional problems, speech comprehension issues). All participants signed an inform consent form before participation with included a 60 CHF reward for participation. The study was approved by the ethics commision of the canton of Zurich.\n",
    "    \n",
    "XX were excluded due to...Data from x participant was discarded...\n",
    "    \n",
    "Before selection for the experiment participants were screened via phone call or email on the main inclusion criteria for the study. Upon arrival to the lab participants first filled in a short questionnaire with a additional questions on language hearing and an relevant psychological or psychiatric condition that may impact task performance. The questionnaire was presented using the web application [Redcap](https://www.project-redcap.org/) to ensure that any personal data was saved in a secure environment. [[[***Enter what variables were masked and list of variables in the appendix ***]]]. After completion of the pilot the survey data was downloaded as a table with identifying variables like D.O.B masked. \n",
    "    \n",
    "After completing the questionnaires participants were informed about the specific content of the session. Electrode placement and signal check took approximately between 20 and 40 minutes. Then a few example trials were presented before starting the session consisting of: a 4 minutes resting state recording with eyes closed, the main task (with 4 blocks of approximately 15 minuntes duration each) and finally,  another 4 minutes of eyes closed resting state. \n",
    "\n",
    "## Experimental Task\n",
    "    \n",
    "### Trial design\n",
    "We used a version of the Coordinate Response Measure (CRM) task used in brungart2001evaluation. In the task, participants hear different versions of a setence with a fixed structure and three words that change in every trial. There are 4 possible alternatives for each of the targets, thus there are 64 possible variations of the sentences. The task was originally designed to investigate intelligibility of speech commands (in a military context) in audio transmissions with various types of background competition. The German sentence used in this study was:\n",
    "    \n",
    "> \"Vorsicht [call sign], geh sofort zum [color] Feld von der Spalte [number]\" \n",
    "    \n",
    "(Translated with a small variation from the original article in English: \"*Ready [call sign] go to [color][number] now*\"). The 4 possible call sign items were: Adler, Drossel, Tiger or Kröte (eagle, thrush, tiger or toad). The possible colors were gelben, gruenen, roten, weissen (yellow, green, red or white). The numbers were Ein, Zwei, Drei or Vier (one to four). \n",
    "    \n",
    "        [[[ insert screenshot about here ]]] \n",
    "    \n",
    "In each trial the sentences were aurally presented and a fixation cross was shown in the center of the screen. After playing the sentence, an array of 4 x 3 was shown with drawings representing the target items, the array had always in the same order and columns 1-3 represented targets 1-3 in the same order as they appeared in the sentence, that is, first the column with call signs, then colors and then numbers. Participants had to click with the mouse on the picture representing the item they heard in each of the columns before moving on to the next trial. The trials were response-terminated, but there was a response time limit of 10 seconds before moving on to the next trial. In the example trials, presented with 3 different levels of auditory difficulty the researchers conducting the experiment ensured that participants understood that they should click on each of the three columns in all trials, even if they were not sure what item they heard. \n",
    "\n",
    "### Blocks design\n",
    "The sentences were presented in blocks of two audio manipulation conditions: noise vocoded (NV) or with a background speech-shaped noise (SSN) and there were three possible levels of difficulty. Half of the sentences were synthesized with a female voice and half with a male voice. The *Stimuli* section provides more details on these manipulations.\n",
    "    \n",
    "The task consisted of 4 blocks of trials: two blocks of NV and two blocks of SSN stimuli presented in alternating order. There were four different block orders (all possible block sequences without consecutive blocks of the same condition, e.g., NV1-SSN1-NV2-SSN2). Participants were randomly assigned to these sequences [[[[[[CHECK - ref to code]]]]]. Within in each block the distribution of voices and levels of difficulty [[[[...]]]\n",
    "\n",
    "\n",
    "    \n",
    "### Stimuli \n",
    "### Speech generation \n",
    "We used the [Google Cloud text-to-speech API](https://cloud.google.com/text-to-speech/) to generate the 64 possible variations of our sentence in a female and male speaker voice. Each sentence was output in a separate .wav file with 48KHz sampling rate. After trying several of the freely available voices we chose the [neural2 voices](https://cloud.google.com/text-to-speech/docs/voice-types#neural2_voices) 'de-DE-Neural2-D' and 'de-DE-Neural2-F' for male and female speech, respective, as they sounded more natural (this was verified by native speakers). We used a Python script to communicate with the text-to-speech API (see      \n",
    "[Gen_stimuli script](https://github.com/Neuroling/SPINCO_SINEEG/blob/65c604a2246d26b94cdd150298f1dd7150bd728f/Gen_stimuli/Gen_speech/TTS_sentenceGenerator.py) in our project repository) \n",
    "\n",
    "    Afte\n",
    "  \n",
    "#### Noise vocoding\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "## Resting-state \n",
    "The resting-state recordings before and after the task were identical. Subjects were instructed to close their eyes and remain still for 4 minutes. The start and end of the 4 minutes block was indicated by a beep sound (1-second long). \n",
    "   \n",
    "## EEG\n",
    "### Signal acquisition \n",
    "The EEG recordings were conducted using the Biosemi Active Two MK2hS-System with a sampling rate of 2048 Hz. We used 64 scalp electrodes positioned according to the 10-20 system. In addition, six external Flat-Type Active electrodes were used: four electrodes for recording the vertical and horizontal electro-oculogram (EOG) and two electrodes were placed at mastoids for off-line reference. The Biosemi system uses two additional electrodes (Common Mode Sense [CMS] and Driven Right Leg [DRL]) creating a feedback loop to replace the conventional ground electrode (see www.biosemi.com/faq/cms&drl.htm for details). The CMS electrode served as online reference. The offset of the electrodes was kept between +20 and -20 µV.\n",
    " \n",
    "    \n",
    "### Minimal preprocessing of source data and event trigger correction\n",
    "The source data directly obtained from the recordings underwent minimal preprocessing and correction of event triggers in order to generate the raw datasets used for further preprocessing and analysis. \n",
    "    \n",
    "During the experimental session a single file was generated containing the resting-state recordings as well as the main task. The file was  in Biosemi's [bdf](https://www.biosemi.com/faq/file_format.htm) format which is a 24 bits version of the popular [EDF](https://www.edfplus.info/#:~:text=European%20Data%20Format%20(EDF)&text=The%20European%20Data%20Format%20(EDF,international%20Sleep%20Congress%20in%20Copenhagen.) format. Using EEGlab we \n",
    "    \n",
    "    \n",
    "    \n",
    "### Preprocessing\n",
    "We opted for a fully automated pipeline implemented with the open-source Matlab toolbox [automagic](https://github.com/methlabUZH/automagic), see (@pedroni2019automagic). The toolbox is actually a wrapper that runs different functions from EEGlab (@delorme2004eeglab), probably the most widely used open-source software for EEG analysis (also Matlab-based). Automagic facilitates selection of multiple options for data cleaning and artifact rejection available in different plugins for EEGlab. In addition, it provides its own data quality summary measures. We chose Automagic as our main preprocessing approach first to facilitate automation, potentially iterating through different pipeline variations and second, to obtain more  *objective* descriptors of data quality,  constraining the researchers' degrees of freedom and facilitating transparency on subsequent decisions and explorations on data quality levels vis-a-vis main analyeses results. \n",
    "    \n",
    "    \n",
    "# Potential secondary analyses   \n",
    "## Exploring data qualities and minimal preprocessing\n",
    "-   Neurofeedback studies deal with very noisy data in real-time and this requires some basic preprocessing to ensure we are targetting the intenteded neural processes, or at least approximating them as much as possible. Thus a potential question is how different preprocessing pipelines may affect our main analysis results (e.g., multivariate pattern analysis and classification)\n",
    "\n",
    "-   A related question is how individual differences in data quality before/after preprocessing may affect performance of classifiers\n",
    "    \n",
    "## Utilizing the resting-state recordings\n",
    "-   Explore individual alpha peaks (IAF) in resting state recordings. Use IAF-based band as target for decoding. If neurofeedback settings are to work out, these individual differences would be important. For example, we may not be able to adequately capture alpha activity in all subjects, due to SNR or to actual differences in oscillatory activity. Here the emphasis is on alpha as an example of typically clear feature, but this may apply to other features or frequency ranges.  \n",
    "    \n",
    "-   Explore consistency of IAF (before and after task). This can give an impression of robustness of features like individual alpha peak and by extension of using power averaged by the classical frequency bands.\n",
    "    \n",
    "-   Explore correlations between resting state power and task performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
